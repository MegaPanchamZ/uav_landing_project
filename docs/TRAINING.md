# üìñ Training Guide

This guide covers the complete training pipeline for the Ultra-Fast UAV Landing Detection model.

## üîÑ Staged Training Pipeline

Our training follows a **3-stage progressive fine-tuning** approach:

```
Stage 0: BiSeNetV2 (Cityscapes) ‚Üí Baseline Segmentation
Stage 1: DroneDeploy Dataset ‚Üí Aerial View Adaptation  
Stage 2: UDD6 Dataset ‚Üí Landing-Specific Classes
```

### Why Staged Training?

1. **Transfer Learning**: Leverages pre-trained features
2. **Domain Adaptation**: Gradually adapts from ground-level to aerial view
3. **Task Specialization**: Final stage focuses on landing detection

## üìä Datasets

### Stage 1: DroneDeploy Dataset
- **Size**: 55 images (44 train, 11 val)
- **Classes**: 7 (Background, Building, Road, Trees, Car, Pool, Other)
- **Purpose**: Intermediate aerial view adaptation
- **Resolution**: Variable, resized to 256√ó256

### Stage 2: UDD6 Dataset  
- **Size**: 106 train samples, 35 validation
- **Classes**: 6 ‚Üí mapped to 4 landing classes
- **Purpose**: Final landing site specialization
- **Resolution**: Variable, resized to 256√ó256

## üèóÔ∏è Model Architecture

### Ultra-Fast BiSeNet

```python
class UltraFastBiSeNet(nn.Module):
    def __init__(self, num_classes=7):
        super().__init__()
        
        # Ultra-lightweight backbone
        self.backbone = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1, bias=False),    # 256√ó256
            nn.BatchNorm2d(32), nn.ReLU(inplace=True),
            
            nn.Conv2d(32, 64, 3, stride=2, padding=1, bias=False),  # 128√ó128
            nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            
            nn.Conv2d(64, 128, 3, stride=2, padding=1, bias=False), # 64√ó64
            nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            
            nn.Conv2d(128, 128, 3, padding=1, bias=False),
            nn.BatchNorm2d(128), nn.ReLU(inplace=True),
        )
        
        # Simple decoder
        self.decoder = nn.Sequential(
            nn.Conv2d(128, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, 3, padding=1, bias=False),
            nn.BatchNorm2d(32), nn.ReLU(inplace=True),
        )
        
        self.classifier = nn.Conv2d(32, num_classes, 1)
```

**Key Features:**
- **Parameters**: 333,668 (vs millions in full BiSeNet)
- **Input Size**: 256√ó256 (vs 512√ó512 for speed)
- **Architecture**: Encoder-decoder with skip connections
- **Optimizations**: No bias in conv layers, minimal channels

## ‚ö° Training Optimizations

### 8GB GPU Optimizations

```python
# Memory optimizations
BATCH_SIZE = 6          # Reduced from 16
INPUT_SIZE = 256        # Reduced from 512
PERSISTENT_WORKERS = True
PIN_MEMORY = True

# Mixed precision training
from torch.amp import autocast, GradScaler
scaler = GradScaler('cuda')

with autocast('cuda'):
    outputs = model(images)
    loss = criterion(outputs, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### Speed Optimizations

- **DataLoader**: `num_workers=4, persistent_workers=True`
- **Model**: Smaller architecture with fewer parameters
- **Training**: Fewer epochs (6 + 8 vs 20+ each)
- **Mixed Precision**: CUDA AMP for 2x speedup

## üéØ Training Configuration

### Stage 1: DroneDeploy Fine-tuning

```python
# Learning rate schedule
INITIAL_LR = 1e-3
LR_SCHEDULE = 'cosine'
EPOCHS = 6

# Data augmentation
transforms = A.Compose([
    A.RandomRotate90(p=0.3),
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.3),
    A.Normalize(mean=[0.485, 0.456, 0.406], 
                std=[0.229, 0.224, 0.225])
])

# Loss function
criterion = nn.CrossEntropyLoss(
    ignore_index=255,
    weight=class_weights
)
```

### Stage 2: UDD6 Fine-tuning

```python
# Lower learning rate for fine-tuning
INITIAL_LR = 5e-4
EPOCHS = 8

# Class mapping (UDD6 ‚Üí Landing Classes)
UDD_TO_LANDING = {
    0: 0,  # Other ‚Üí Background
    1: 3,  # Facade ‚Üí Danger
    2: 1,  # Road ‚Üí Safe
    3: 2,  # Vegetation ‚Üí Caution
    4: 3,  # Vehicle ‚Üí Danger
    5: 2,  # Roof ‚Üí Caution
}
```

## üìà Training Results

### Stage 1 Results
- **Final Validation Loss**: 0.946
- **Training Time**: ~12 minutes
- **Speed**: ~14.7s/epoch initially ‚Üí ~12s/epoch

### Stage 2 Results
- **Final Validation Loss**: 0.738
- **IoU Score**: 59.0%
- **Training Time**: ~13 minutes
- **Speed**: ~2.4s/iteration

### Performance Metrics
- **Total Training Time**: 25 minutes
- **Memory Usage**: <6GB VRAM
- **Final Model Size**: 1.3 MB

## üõ†Ô∏è Running Training

### Prerequisites

```bash
# Install dependencies
pip install torch torchvision albumentations tqdm

# Ensure datasets are available
# DroneDeploy: ../datasets/drone_deploy_dataset_intermediate/
# UDD6: ../datasets/UDD/UDD/UDD6/
```

### Command

```bash
# Run ultra-fast training
cd scripts/
python ultra_fast_training.py

# Monitor training
# Watch GPU usage: nvidia-smi
# Check progress: tail -f training.log
```

### Output Files

```
trained_models/
‚îú‚îÄ‚îÄ ultra_stage1_best.pth      # DroneDeploy fine-tuned
‚îú‚îÄ‚îÄ ultra_stage2_best.pth      # Final UDD6 model ‚≠ê
‚îî‚îÄ‚îÄ ultra_fast_uav_landing.onnx # ONNX export
```

## üîß Troubleshooting

### Memory Issues

```bash
# If CUDA OOM:
# 1. Reduce batch size in ultra_fast_training.py
BATCH_SIZE = 4  # or even 2

# 2. Enable gradient checkpointing
torch.utils.checkpoint.checkpoint(model, x)

# 3. Clear cache
torch.cuda.empty_cache()
```

### Slow Training

```bash
# Check GPU utilization
nvidia-smi

# Enable persistent workers
persistent_workers=True

# Increase num_workers if CPU allows
num_workers=6
```

### Poor Results

```python
# Adjust learning rate
INITIAL_LR = 1e-4  # Lower for stability

# Add data augmentation
A.RandomCrop(224, 224),
A.ColorJitter(brightness=0.2, contrast=0.2)

# Check class balance
print(torch.bincount(labels.flatten()))
```

## üìä Monitoring Training

### Key Metrics

- **Training Loss**: Should decrease steadily
- **Validation Loss**: Should follow training loss
- **IoU Score**: Should increase (target >50%)
- **Training Speed**: Should be ~2-3s/iteration

### Expected Progress

```
Epoch 1: Loss ~1.9 ‚Üí ~1.4
Epoch 2: Loss ~1.4 ‚Üí ~1.2  
Epoch 3: Loss ~1.2 ‚Üí ~1.0
...
Final: Loss ~0.7, IoU ~59%
```

## üéØ Advanced Topics

### Custom Datasets

```python
class CustomDataset(Dataset):
    def __init__(self, image_dir, label_dir, transform=None):
        self.images = list(Path(image_dir).glob("*.jpg"))
        self.labels = list(Path(label_dir).glob("*.png"))
        self.transform = transform
        
    def __getitem__(self, idx):
        image = cv2.imread(str(self.images[idx]))
        label = cv2.imread(str(self.labels[idx]), 0)
        
        if self.transform:
            augmented = self.transform(image=image, mask=label)
            image, label = augmented['image'], augmented['mask']
            
        return torch.from_numpy(image).float().permute(2,0,1) / 255.0, \
               torch.from_numpy(label).long()
```

### Hyperparameter Tuning

```python
# Grid search example
learning_rates = [1e-3, 5e-4, 1e-4]
batch_sizes = [4, 6, 8]
architectures = ['ultra-fast', 'fast', 'accurate']

for lr in learning_rates:
    for bs in batch_sizes:
        # Run training with these params
        train_model(lr=lr, batch_size=bs)
```

### Model Quantization

```python
# Post-training quantization
import torch.quantization as quant

model_quantized = quant.quantize_dynamic(
    model, {nn.Linear, nn.Conv2d}, dtype=torch.qint8
)

# Size reduction: ~4x smaller
# Speed: 2-3x faster on CPU
```

## üöÄ Next Steps

1. **Experiment** with different architectures
2. **Collect** more training data
3. **Fine-tune** hyperparameters
4. **Deploy** to edge devices
5. **Optimize** for specific hardware

---

**Happy Training!** üéØ
